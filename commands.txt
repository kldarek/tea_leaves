# Start training script: 

python train.py \
  --model_name_or_path xlm-roberta-base \
  --dataset_name allegro/klej-cbd  \
  --do_train \
  --max_seq_length 128 \
  --per_device_train_batch_size 16 \
  --learning_rate 2e-5 \
  --num_train_epochs 5 \
  --output_dir /tmp/cbd/ \
  --evaluation_strategy steps \
  --eval_steps 50 \
  --logging_strategy steps \
  --logging_steps 50 \
  --report_to wandb \
  --seed 42 \
  --overwrite_output_dir \
  --aug_prob 0.02

# Start W&B Sweep:

wandb sweep sweep.yaml

